{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0301"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (1.11.0)\n",
      "Requirement already satisfied: typing-extensions in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from torch) (4.0.1)\n",
      "\u001b[33mWARNING: You are using pip version 21.3.1; however, version 22.0.4 is available.\n",
      "You should consider upgrading via the '/usr/local/bin/python3.10 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Current Standard Directory is:  /Users/rossbarker/Library/CloudStorage/OneDrive-Personal/02_Fertapp/01_Python_script\n",
      "New working directory is:  /Users/rossbarker/Library/CloudStorage/OneDrive-Personal/02_Fertapp\n"
     ]
    }
   ],
   "source": [
    "import contextualized_topic_models\n",
    "from contextualized_topic_models.models.ctm import ZeroShotTM, CombinedTM\n",
    "from contextualized_topic_models.utils.data_preparation import TopicModelDataPreparation\n",
    "from contextualized_topic_models.utils.preprocessing import WhiteSpacePreprocessingStopwords\n",
    "import nltk\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd    \n",
    "import os\n",
    "import pickle\n",
    "cwd = os.getcwd()\n",
    "print('Current Standard Directory is: ', cwd)\n",
    "#absolute_path = 'C:/Users/rbarker/OneDrive/02_Fertapp' # for work PC\n",
    "absolute_path = '/Users/rossbarker/Library/CloudStorage/OneDrive-Personal/02_Fertapp' # for macbook\n",
    "os.chdir(absolute_path)\n",
    "print('New working directory is: ', os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### import my file of cleaned, multilingual data from R script\n",
    "filename = '03_data/02_output/0100_reliable_langdetection.csv' # load dataset of reviews \n",
    "fullsample = pd.read_csv(filename, usecols = [\"app\", \"review\", \"language\", \"date\", \"rating\"], header='infer', encoding=\"utf-8\") \n",
    "fullsample.columns = ['id', 'text', 'language', 'date', 'rating'] # rename columns \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "spanishsample = fullsample[fullsample['language'] == \"Spanish\"]\n",
    "nonspanishsample = fullsample[fullsample['language'] != \"Spanish\"] # selecting all non-spanish texts from the earlier sample\n",
    "test_docs = nonspanishsample['text'].tolist() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_seeds():\n",
    "  torch.manual_seed(10)\n",
    "  torch.cuda.manual_seed(10)\n",
    "  np.random.seed(10)\n",
    "  random.seed(10)\n",
    "  torch.backends.cudnn.enabled = False\n",
    "  torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading stopwords: <urlopen error [SSL:\n",
      "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
      "[nltk_data]     unable to get local issuer certificate (_ssl.c:997)>\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "### Preprocessing\n",
    "from nltk.corpus import stopwords as stop_words\n",
    "nltk.download('stopwords')\n",
    "stopwords = list(set(stop_words.words('spanish')))\n",
    "\n",
    "# from 10_fertapp_cotis_python\n",
    "new_stopwords = [\"im\", \"x\", \"xx\", \"xxx\", \"xxxx\", \"xo\", \"xoxo\", \"xox\", \"day\",\n",
    "                 \"ovia\", \"ladytimer\", \"clue\", \"pinkbird\", \"clue\", \"Ovia\",\n",
    "                 \"clover\", \"womanlog\", \"fertility friend\", \"woom\",\n",
    "                 \"tempdrop\", \"femm\", \"glow\", \"maya\", \"natural cycles\", \"ava\",\n",
    "                 \"kindara\", \"flo\", # these from fertility apps\n",
    "                 \"app\", \"apps\", \"application\", \"applications\", \"nurx\"]\n",
    "\n",
    "stopwords = list(set(stopwords+new_stopwords)) # combine the two lists, the base and the custom stop words\n",
    "\n",
    "documents = spanishsample.text.tolist()\n",
    "sp = WhiteSpacePreprocessingStopwords(documents, stopwords_list=stopwords, min_words=3, remove_numbers=True,\n",
    "                             max_df=0.4) \n",
    "preprocessed_documents, unpreprocessed_corpus, vocab = sp.preprocess()\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import pickle\n",
    "df_unpreprocessed_corpus = pd.DataFrame(unpreprocessed_corpus)\n",
    "df_spanishsample = pd.DataFrame(spanishsample)\n",
    "df_unpreprocessed_corpus.columns = ['text']\n",
    "\n",
    "keys = list(df_unpreprocessed_corpus.columns.values)\n",
    "i1 = df_spanishsample.set_index(keys).index\n",
    "i2 = df_unpreprocessed_corpus.set_index(keys).index\n",
    "toremove = df_spanishsample[~i1.isin(i2)]\n",
    "toremove = list(toremove['text'])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "scriptfile = '0300_'\n",
    "folder = '03_data/02_output/01_spanish/'\n",
    "language = \"spanish_\"\n",
    "spanishsample = spanishsample[-spanishsample[\"text\"].isin(toremove)]\n",
    "spanishsample.to_csv(folder  + scriptfile + language + \"df_spanishsample.csv\",index=False)\n",
    "pickle.dump(spanishsample, open(folder + scriptfile + \"spanishsample.pkl\", \"wb\")) # this is necessary, as we load them lare\n",
    "pickle.dump(nonspanishsample, open(folder + scriptfile + \"nonspanishsample.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a95dbce63d374da587fe0a3b02bad542",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/263 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "zero_tp = TopicModelDataPreparation(\"paraphrase-multilingual-mpnet-base-v2\") # we use a multilingual for good measure (50+ languages)\n",
    "zero_training_dataset = zero_tp.fit(text_for_contextual=unpreprocessed_corpus, text_for_bow=preprocessed_documents) \n",
    "# zero_tp.vocab # to check our words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the model\n",
    "scriptfile = '0301_'\n",
    "folder = '03_data/02_output/01_spanish/'\n",
    "topicnum = '33'\n",
    "topicnumber = 33\n",
    "zero_ctm = ZeroShotTM(bow_size=len(zero_tp.vocab), contextual_size=768,\n",
    "                      n_components=topicnumber, num_epochs=50, batch_size = topicnumber) # we get an error with windows, but it's fine with mac\n",
    "zero_ctm.fit(zero_training_dataset) #, n_samples=1) \n",
    "\n",
    "pickle.dump(zero_ctm, open(folder + scriptfile + topicnum +language + \"k_zero_ctm.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Spanish topic prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "spanish_topics_predictions = zero_ctm.training_doc_topic_distributions\n",
    "scriptfile = '0301_'\n",
    "folder = '03_data/02_output/01_spanish/'\n",
    "pickle.dump(spanish_topics_predictions, open(folder + scriptfile + topicnum + language +\"spanish_topics_predictions.pkl\", \"wb\")) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-Spanish topic prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/contextualized_topic_models/utils/data_preparation.py:98: UserWarning: The method did not have in input the text_for_bow parameter. This IS EXPECTED if you are using ZeroShotTM in a cross-lingual setting\n",
      "  warnings.warn(\"The method did not have in input the text_for_bow parameter. This IS EXPECTED if you \"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8560ba48da046dbb014a20c94494d3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/2132 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling: [1/1]: : 1it [01:22, 82.84s/it]\n"
     ]
    }
   ],
   "source": [
    "zero_tp = TopicModelDataPreparation(\"paraphrase-multilingual-mpnet-base-v2\")\n",
    "testing_dataset = zero_tp.transform(test_docs)\n",
    "\n",
    "nonspanish_topics_predictions = zero_ctm.get_thetas(testing_dataset, n_samples=1) \n",
    "\n",
    "pickle.dump(nonspanish_topics_predictions, open(folder + scriptfile  + topicnum + \"nonspanish_topics_predictions.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0202 - import/export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Standard Directory is:  /Users/rossbarker/Library/CloudStorage/OneDrive-Personal/02_Fertapp\n",
      "New working directory is:  /Users/rossbarker/Library/CloudStorage/OneDrive-Personal/02_Fertapp\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import os\n",
    "cwd = os.getcwd()\n",
    "print('Current Standard Directory is: ', cwd)\n",
    "#absolute_path = 'C:/Users/rbarker/OneDrive/02_Fertapp' \n",
    "absolute_path = '/Users/rossbarker/Library/CloudStorage/OneDrive-Personal/02_Fertapp' # for macbook\n",
    "os.chdir(absolute_path)\n",
    "print('New working directory is: ', os.getcwd())\n",
    "\n",
    "sourcescriptfile = \"0301_\"\n",
    "folder = '03_data/02_output/01_spanish/'\n",
    "topicnum = '33'\n",
    "language = 'spanish_'\n",
    "zero_ctm = pickle.load(open(folder + sourcescriptfile + topicnum +language + \"k_zero_ctm.pkl\", \"rb\"))\n",
    "nonspanish_topics_predictions = pickle.load(open(folder + sourcescriptfile + topicnum + \"nonspanish_topics_predictions.pkl\", \"rb\"))\n",
    "spanish_topics_predictions = pickle.load(open(folder + sourcescriptfile +  topicnum +language + \"spanish_topics_predictions.pkl\", \"rb\"))\n",
    "mytopic_lists = zero_ctm.get_topic_lists(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Exporing the matrices\n",
    "import pandas as pd\n",
    "scriptfile = \"0301_\"\n",
    "topicnum = '33'\n",
    "typeofsource = 'reviews'\n",
    "\n",
    "## to DF\n",
    "nonspanish_topics_predictions = pd.DataFrame(nonspanish_topics_predictions)\n",
    "spanish_topics_predictions = pd.DataFrame(spanish_topics_predictions)\n",
    "mytopic_lists = pd.DataFrame(mytopic_lists)\n",
    "\n",
    "# export\n",
    "nonspanish_topics_predictions.to_csv(folder + scriptfile + topicnum + typeofsource + \"nonspanish_topics_predictions.csv\",index=False)\n",
    "spanish_topics_predictions.to_csv(folder + scriptfile + topicnum + typeofsource +language + \"spanish_topics_predictions.csv\",index=False)\n",
    "mytopic_lists.to_csv(folder + scriptfile + topicnum + typeofsource +language +  \"mytopic_lists.csv\",index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "sourcescriptfile = '0300_'\n",
    "spanishsample = pickle.load(open(folder + sourcescriptfile + \"spanishsample.pkl\", \"rb\"))\n",
    "nonspanishsample = pickle.load(open(folder + sourcescriptfile + \"nonspanishsample.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "### adding a uniique identifier\n",
    "spanishsample[\"comparableid\"] = spanishsample.index # getting my row index\n",
    "spanishsample = spanishsample.applymap(str)\n",
    "spanishsample['newcomparableid'] = spanishsample.comparableid + spanishsample.id + spanishsample.language\n",
    "\n",
    "nonspanishsample[\"comparableid\"] = nonspanishsample.index # getting my row index\n",
    "nonspanishsample = nonspanishsample.applymap(str)\n",
    "nonspanishsample['newcomparableid'] = nonspanishsample.comparableid + nonspanishsample.id + nonspanishsample.language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "spanishsample = pd.DataFrame(spanishsample)\n",
    "spanishsample['row_num'] = spanishsample.reset_index().index\n",
    "spanish_topics_predictions = pd.DataFrame(spanish_topics_predictions)\n",
    "spanish_topics_predictions['row_num'] = spanish_topics_predictions.reset_index().index\n",
    "spanish = spanishsample.merge(spanish_topics_predictions, on='row_num')\n",
    "\n",
    "\n",
    "nonspanishsample = pd.DataFrame(nonspanishsample)\n",
    "nonspanishsample['row_num'] = nonspanishsample.reset_index(drop=True).index\n",
    "nonspanish_topics_predictions = pd.DataFrame(nonspanish_topics_predictions)\n",
    "nonspanish_topics_predictions['row_num'] = nonspanish_topics_predictions.reset_index().index\n",
    "nonspanish = nonspanishsample.merge(nonspanish_topics_predictions, on='row_num')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "### here, we export \n",
    "scriptfile = \"0302_\"\n",
    "folder = '03_data/02_output/01_spanish/'\n",
    "topicnum = '33'\n",
    "typeofsource = 'reviews'\n",
    "spanish.to_csv(folder + scriptfile + topicnum + typeofsource + \"spanish.csv\",index=False)\n",
    "nonspanish.to_csv(folder + scriptfile + topicnum + typeofsource + \"nonspanish.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
