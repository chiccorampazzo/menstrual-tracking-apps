{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Setup (run on 10 python)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rossbarker/.pyenv/versions/3.10.1/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "import contextualized_topic_models\n",
    "from contextualized_topic_models.models.ctm import ZeroShotTM, CombinedTM\n",
    "from contextualized_topic_models.utils.data_preparation import TopicModelDataPreparation\n",
    "from contextualized_topic_models.utils.preprocessing import WhiteSpacePreprocessingStopwords\n",
    "import nltk\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd    \n",
    "import os\n",
    "import pickle\n",
    "cwd = os.getcwd()\n",
    "absolute_path = [INSERT PATH HERE]\n",
    "\n",
    "os.chdir(absolute_path)\n",
    "language = \"English\" # here set the langauge we are focussing on\n",
    "languagelowercase = \"english\"\n",
    "typeofsource = \"reviews\"\n",
    "scriptfile = f\"02_{language}\"\n",
    "topicnum = '29'\n",
    "topicnumber = 29\n",
    "folder = \"03_data/02_output/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### import my file of cleaned, multilingual data from R script\n",
    "filename = '03_data/02_output/0100_reliable_langdetection.csv' # load dataset of reviews \n",
    "fullsample = pd.read_csv(filename, usecols = [\"app\", \"review\", \"language\", \"date\", \"rating\"], header='infer', encoding=\"utf-8\") \n",
    "fullsample.columns = ['id', 'text', 'language', 'date', 'rating'] # rename columns \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Earliest date entry:\n",
      "id                                                   WomanLog\n",
      "text        Its a good app. It would be nice to have an en...\n",
      "language                                              English\n",
      "date                                      1970-01-01 00:00:00\n",
      "rating                                             2011-01-01\n",
      "Name: 162124, dtype: object\n",
      "\n",
      "Latest date entry:\n",
      "id                                           Fertility Friend\n",
      "text        This app is FANTASTIC! Along with being able t...\n",
      "language                                              English\n",
      "date                            1970-01-01 00:00:00.000000005\n",
      "rating                                             2013-03-01\n",
      "Name: 239435, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Convert the 'date' column to datetime if it's not already\n",
    "fullsample['date'] = pd.to_datetime(fullsample['date'])\n",
    "\n",
    "# Sort the DataFrame by the 'date' column\n",
    "df_sorted = fullsample.sort_values(by='date')\n",
    "\n",
    "# Display the earliest and latest dates\n",
    "earliest_date = df_sorted.iloc[0]\n",
    "latest_date = df_sorted.iloc[-1]\n",
    "\n",
    "# Display the sorted DataFrame\n",
    "print(\"Earliest date entry:\")\n",
    "print(earliest_date)\n",
    "\n",
    "print(\"\\nLatest date entry:\")\n",
    "print(latest_date)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = fullsample[fullsample['language'] == language] # selecting all traininglanguage (or whicher sample we are using to train the model) \n",
    "nonsample = fullsample[fullsample['language'] != language] \n",
    "test_docs = nonsample['text'].tolist() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_seeds():\n",
    "  torch.manual_seed(10)\n",
    "  torch.cuda.manual_seed(10)\n",
    "  np.random.seed(10)\n",
    "  random.seed(10)\n",
    "  torch.backends.cudnn.enabled = False\n",
    "  torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Preprocessing\n",
    "from nltk.corpus import stopwords as stop_words\n",
    "nltk.download('stopwords')\n",
    "stopwords = list(set(stop_words.words(languagelowercase)))\n",
    "\n",
    "new_stopwords = [\"im\", \"x\", \"xx\", \"xxx\", \"xxxx\", \"xo\", \"xoxo\", \"xox\", \"day\",\n",
    "                 \"ovia\", \"ladytimer\", \"clue\", \"pinkbird\", \"clue\", \"Ovia\",\n",
    "                 \"clover\", \"womanlog\", \"fertility friend\", \"woom\",\n",
    "                 \"tempdrop\", \"femm\", \"glow\", \"maya\", \"natural cycles\", \"ava\",\n",
    "                 \"kindara\", \"flo\",\n",
    "                 \"app\", \"apps\", \"application\", \"applications\", \"nurx\"]\n",
    "\n",
    "stopwords = list(set(stopwords+new_stopwords)) # combine the two lists, the base and the custom stop words\n",
    "\n",
    "documents = sample.text.tolist()\n",
    "sp = WhiteSpacePreprocessingStopwords(documents, stopwords_list=stopwords, min_words=3, remove_numbers=True,\n",
    "                             max_df=0.4) \n",
    "preprocessed_documents, unpreprocessed_corpus, vocab = sp.preprocess()\n",
    "\n",
    "df_unpreprocessed_corpus = pd.DataFrame(unpreprocessed_corpus)\n",
    "df_sample = pd.DataFrame(sample)\n",
    "df_unpreprocessed_corpus.columns = ['text']\n",
    "\n",
    "keys = list(df_unpreprocessed_corpus.columns.values)\n",
    "i1 = df_sample.set_index(keys).index\n",
    "i2 = df_unpreprocessed_corpus.set_index(keys).index\n",
    "toremove = df_sample[~i1.isin(i2)]\n",
    "toremove = list(toremove['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = sample[-sample[\"text\"].isin(toremove)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_tp = TopicModelDataPreparation(\"paraphrase-multilingual-mpnet-base-v2\") # we use a multilingual for good measure (50+ languages)\n",
    "zero_training_dataset = zero_tp.fit(text_for_contextual=unpreprocessed_corpus, text_for_bow=preprocessed_documents) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decide on the number of topics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from contextualized_topic_models.evaluation.measures import CoherenceNPMI, InvertedRBO\n",
    "corpus = [d.split() for d in preprocessed_documents]\n",
    "num_topics = [4,5,6,7,8,9,\n",
    "10, 11, 12, 13, 14, 15, 16, 17, 18, 19, \n",
    "20, 21, 22, 23, 24, 25, 26, 27, 28, 29,\n",
    "30, 31, 32, 33, 34, 35, 36, 37, 38, 39, \n",
    "40, 41, 42, 43, 44, 45, 46, 47, 48, 49,\n",
    "50, 51, 52, 53, 54, 55, 56, 57, 58,\n",
    "59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80] \n",
    "\n",
    "\n",
    "num_runs = 1 \n",
    "\n",
    "\n",
    "best_topic_coherence = -999\n",
    "best_num_topics = 0\n",
    "for n_components in num_topics:\n",
    "  for i in range(num_runs):\n",
    "    print(\"num topics:\", n_components, \"/ num run:\", i)\n",
    "    zero_ctm = ZeroShotTM(bow_size=len(zero_tp.vocab), contextual_size=768, \n",
    "                     n_components=n_components, num_epochs=4, batch_size = n_components) \n",
    "    zero_ctm.fit(zero_training_dataset) # run the model\n",
    "    coh = CoherenceNPMI(zero_ctm.get_topic_lists(10), corpus)\n",
    "    coh_score = coh.score()\n",
    "    print(\"coherence score:\", coh_score)\n",
    "    if best_topic_coherence < coh_score:\n",
    "      best_topic_coherence = coh_score\n",
    "      best_num_topics = n_components\n",
    "    print(\"current best coherence\", best_topic_coherence, \"/ best num topics\", best_num_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topicnum = '29'\n",
    "topicnumber = 29\n",
    "\n",
    "zero_ctm = ZeroShotTM(bow_size=len(zero_tp.vocab), contextual_size=768,\n",
    "                      n_components=topicnumber, num_epochs=50, batch_size = topicnumber) \n",
    "zero_ctm.fit(zero_training_dataset)\n",
    "mytopic_lists = zero_ctm.get_topic_lists(10) # get the top 10 words per topic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(zero_ctm, open(f\"{folder}/{scriptfile}_{typeofsource}_{topicnum}_zero_ctm.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Training language topic prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traininglanguage_topics_predictions = zero_ctm.training_doc_topic_distributions\n",
    "#pickle.dump(traininglanguage_topics_predictions, open(f\"{folder}/{scriptfile}_{typeofsource}_{language}_{topicnum}_topics_predictions.pkl\", \"wb\")) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-traininglanguage topic prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_tp = TopicModelDataPreparation(\"paraphrase-multilingual-mpnet-base-v2\")\n",
    "testing_dataset = zero_tp.transform(test_docs)\n",
    "testinglanguage_topics_predictions = zero_ctm.get_thetas(testing_dataset, n_samples=1) \n",
    "\n",
    "#pickle.dump(testinglanguage_topics_predictions, open(f\"{folder}/{scriptfile}_{typeofsource}_{topicnum}_non_topics_predictions.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import/export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Exporing the matrices\n",
    "## to DF\n",
    "testinglanguage_topics_predictions = pd.DataFrame(testinglanguage_topics_predictions)\n",
    "traininglanguage_topics_predictions = pd.DataFrame(traininglanguage_topics_predictions)\n",
    "\n",
    "mytopic_lists = pd.DataFrame(mytopic_lists)\n",
    "\n",
    "# export\n",
    "testinglanguage_topics_predictions.to_csv(folder + scriptfile + topicnum + typeofsource + \"testinglanguage_topics_predictions.csv\",index=False)\n",
    "traininglanguage_topics_predictions.to_csv(folder + scriptfile + topicnum + typeofsource + \"traininglanguage_topics_predictions.csv\",index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-17-438a9be14d09>:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  nonsample[\"comparableid\"] = nonsample.index # getting my row index\n"
     ]
    }
   ],
   "source": [
    "### adding a uniique identifier\n",
    "sample[\"comparableid\"] = sample.index # getting the row index\n",
    "sample = sample.applymap(str)\n",
    "sample['newcomparableid'] = sample.comparableid + sample.id + sample.language\n",
    "\n",
    "nonsample[\"comparableid\"] = nonsample.index # getting the row index\n",
    "nonsample = nonsample.applymap(str)\n",
    "nonsample['newcomparableid'] = nonsample.comparableid + nonsample.id + nonsample.language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = pd.DataFrame(sample)\n",
    "sample['row_num'] = sample.reset_index().index\n",
    "traininglanguage_topics_predictions = pd.DataFrame(traininglanguage_topics_predictions)\n",
    "traininglanguage_topics_predictions['row_num'] = traininglanguage_topics_predictions.reset_index().index\n",
    "traininglanguage = sample.merge(traininglanguage_topics_predictions, on='row_num')\n",
    "\n",
    "nonsample = pd.DataFrame(nonsample)\n",
    "nonsample['row_num'] = nonsample.reset_index(drop=True).index\n",
    "testinglanguage_topics_predictions = pd.DataFrame(testinglanguage_topics_predictions)\n",
    "testinglanguage_topics_predictions['row_num'] = testinglanguage_topics_predictions.reset_index().index\n",
    "testinglanguage = nonsample.merge(testinglanguage_topics_predictions, on='row_num')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export the 3 files that we will use later, namely in the R script\n",
    "traininglanguage.to_csv(f\"{folder}/{scriptfile}_{topicnum}_{typeofsource}_{language}.csv\",index=False)\n",
    "testinglanguage.to_csv(f\"{folder}/{scriptfile}_{topicnum}_{typeofsource}_non_{language}.csv\",index=False)\n",
    "mytopic_lists.to_csv(f\"{folder}/{scriptfile}_{topicnum}_{typeofsource}_mytopic_lists.csv\",index=False)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7e1998ff7f8aa20ada591c520b972326324e5ea05489af9e422744c7c09f6dad"
  },
  "kernelspec": {
   "display_name": "Python 3.10.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
