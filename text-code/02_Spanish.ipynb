{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import contextualized_topic_models\n",
    "from contextualized_topic_models.models.ctm import ZeroShotTM, CombinedTM\n",
    "from contextualized_topic_models.utils.data_preparation import TopicModelDataPreparation\n",
    "from contextualized_topic_models.utils.preprocessing import WhiteSpacePreprocessingStopwords\n",
    "import nltk\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd    \n",
    "import os\n",
    "import pickle\n",
    "cwd = os.getcwd()\n",
    "absolute_path = [INSERT PATH HERE]\n",
    "os.chdir(absolute_path)\n",
    "language = \"Spanish\" \n",
    "languagelowercase = \"spanish\"\n",
    "typeofsource = \"reviews\"\n",
    "scriptfile = f\"02_{language}\"\n",
    "topicnum = '19'\n",
    "topicnumber = 19\n",
    "folder = \"03_data/02_output/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = '03_data/02_output/0100_reliable_langdetection.csv' # load dataset of reviews \n",
    "fullsample = pd.read_csv(filename, usecols = [\"app\", \"review\", \"language\", \"date\", \"rating\"], header='infer', encoding=\"utf-8\") \n",
    "fullsample.columns = ['id', 'text', 'language', 'date', 'rating'] # rename columns \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = fullsample[fullsample['language'] == language] \n",
    "nonsample = fullsample[fullsample['language'] != language] \n",
    "test_docs = nonsample['text'].tolist() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_seeds():\n",
    "  torch.manual_seed(10)\n",
    "  torch.cuda.manual_seed(10)\n",
    "  np.random.seed(10)\n",
    "  random.seed(10)\n",
    "  torch.backends.cudnn.enabled = False\n",
    "  torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading stopwords: <urlopen error [SSL:\n",
      "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
      "[nltk_data]     unable to get local issuer certificate (_ssl.c:997)>\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "## Preprocessing\n",
    "from nltk.corpus import stopwords as stop_words\n",
    "nltk.download('stopwords')\n",
    "stopwords = list(set(stop_words.words(languagelowercase)))\n",
    "\n",
    "new_stopwords = [\"im\", \"x\", \"xx\", \"xxx\", \"xxxx\", \"xo\", \"xoxo\", \"xox\", \"day\",\n",
    "                 \"ovia\", \"ladytimer\", \"clue\", \"pinkbird\", \"clue\", \"Ovia\",\n",
    "                 \"clover\", \"womanlog\", \"fertility friend\", \"woom\",\n",
    "                 \"tempdrop\", \"femm\", \"glow\", \"maya\", \"natural cycles\", \"ava\",\n",
    "                 \"kindara\", \"flo\", \n",
    "                 \"app\", \"apps\", \"application\", \"applications\", \"nurx\"]\n",
    "\n",
    "stopwords = list(set(stopwords+new_stopwords)) # combine the two lists, the base and the custom stop words\n",
    "\n",
    "documents = sample.text.tolist()\n",
    "sp = WhiteSpacePreprocessingStopwords(documents, stopwords_list=stopwords, min_words=3, remove_numbers=True,\n",
    "                             max_df=0.4) \n",
    "preprocessed_documents, unpreprocessed_corpus, vocab = sp.preprocess()\n",
    "\n",
    "df_unpreprocessed_corpus = pd.DataFrame(unpreprocessed_corpus)\n",
    "df_sample = pd.DataFrame(sample)\n",
    "df_unpreprocessed_corpus.columns = ['text']\n",
    "\n",
    "keys = list(df_unpreprocessed_corpus.columns.values)\n",
    "i1 = df_sample.set_index(keys).index\n",
    "i2 = df_unpreprocessed_corpus.set_index(keys).index\n",
    "toremove = df_sample[~i1.isin(i2)]\n",
    "toremove = list(toremove['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = sample[-sample[\"text\"].isin(toremove)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_tp = TopicModelDataPreparation(\"paraphrase-multilingual-mpnet-base-v2\") # we use a multilingual for good measure (50+ languages)\n",
    "zero_training_dataset = zero_tp.fit(text_for_contextual=unpreprocessed_corpus, text_for_bow=preprocessed_documents) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the model\n",
    "zero_ctm = ZeroShotTM(bow_size=len(zero_tp.vocab), contextual_size=768,\n",
    "                      n_components=topicnumber, num_epochs=50, batch_size = topicnumber)\n",
    "zero_ctm.fit(zero_training_dataset)\n",
    "mytopic_lists = zero_ctm.get_topic_lists(10) # get the top 10 words per topic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(zero_ctm, open(f\"{folder}/{scriptfile}_{typeofsource}_{language}_{topicnum}_zero_ctm.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Training language topic prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "traininglanguage_topics_predictions = zero_ctm.training_doc_topic_distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-training language topic prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_tp = TopicModelDataPreparation(\"paraphrase-multilingual-mpnet-base-v2\")\n",
    "testing_dataset = zero_tp.transform(test_docs)\n",
    "testinglanguage_topics_predictions = zero_ctm.get_thetas(testing_dataset, n_samples=1) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import/export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Exporing the matrices\n",
    "## to DF\n",
    "testinglanguage_topics_predictions = pd.DataFrame(testinglanguage_topics_predictions)\n",
    "traininglanguage_topics_predictions = pd.DataFrame(traininglanguage_topics_predictions)\n",
    "mytopic_lists = pd.DataFrame(mytopic_lists)\n",
    "\n",
    "# export\n",
    "testinglanguage_topics_predictions.to_csv(folder + scriptfile + topicnum + typeofsource + \"testinglanguage_topics_predictions.csv\",index=False)\n",
    "traininglanguage_topics_predictions.to_csv(folder + scriptfile + topicnum + typeofsource +language + \"traininglanguage_topics_predictions.csv\",index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### adding a uniique identifier\n",
    "sample[\"comparableid\"] = sample.index # getting my row index\n",
    "sample = sample.applymap(str)\n",
    "sample['newcomparableid'] = sample.comparableid + sample.id + sample.language\n",
    "\n",
    "nonsample[\"comparableid\"] = nonsample.index # getting my row index\n",
    "nonsample = nonsample.applymap(str)\n",
    "nonsample['newcomparableid'] = nonsample.comparableid + nonsample.id + nonsample.language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = pd.DataFrame(sample)\n",
    "sample['row_num'] = sample.reset_index().index\n",
    "traininglanguage_topics_predictions = pd.DataFrame(traininglanguage_topics_predictions)\n",
    "traininglanguage_topics_predictions['row_num'] = traininglanguage_topics_predictions.reset_index().index\n",
    "traininglanguage = sample.merge(traininglanguage_topics_predictions, on='row_num')\n",
    "\n",
    "nonsample = pd.DataFrame(nonsample)\n",
    "nonsample['row_num'] = nonsample.reset_index(drop=True).index\n",
    "testinglanguage_topics_predictions = pd.DataFrame(testinglanguage_topics_predictions)\n",
    "testinglanguage_topics_predictions['row_num'] = testinglanguage_topics_predictions.reset_index().index\n",
    "testinglanguage = nonsample.merge(testinglanguage_topics_predictions, on='row_num')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export the 3 files that we will use later \n",
    "traininglanguage.to_csv(f\"{folder}/{scriptfile}_{topicnum}_{typeofsource}_{language}.csv\",index=False)\n",
    "testinglanguage.to_csv(f\"{folder}/{scriptfile}_{topicnum}_{typeofsource}_non_{language}.csv\",index=False)\n",
    "mytopic_lists.to_csv(f\"{folder}/{scriptfile}_{topicnum}_{typeofsource}_{language}_mytopic_lists.csv\",index=False)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7e1998ff7f8aa20ada591c520b972326324e5ea05489af9e422744c7c09f6dad"
  },
  "kernelspec": {
   "display_name": "Python 3.10.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
